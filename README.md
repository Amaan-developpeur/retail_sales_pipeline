# **Retail Sales Analytics & Monitoring Pipeline**  
*A fully automated, production-structured, local-first data engineering system powering real-time retail insights, monitoring, and alerting.*

---

## **Project Overview**

The **Retail Sales Analytics & Monitoring Pipeline** is an end-to-end data engineering solution that mimics how modern retail organizations collect, validate, transform, store, analyze, and monitor daily sales data.

Built entirely in Python and SQLite, it delivers:

- Automated ETL  
- KPI-rich analytics  
- FastAPI endpoints  
- Streamlit dashboards  
- Continuous monitoring  
- Slack alerting on failures & anomalies  

This project demonstrates **real-world data engineering principles**: modularity, observability, automation, idempotency, and business alignment â€” all locally, without cloud dependencies.

---

## **Business Impact**

| Business Problem | Implemented Solution | Value Delivered |
|------------------|----------------------|------------------|
| Scattered CSVs & messy data | Schema-validated ingestion | Reliable single source of truth |
| Manual daily reporting | APScheduler-based automation | Zero manual effort, daily refresh |
| Lack of visibility | Streamlit dashboards | Real-time business insights |
| Undetected data breaks | Monitoring + Slack alerts | Immediate detection of issues |
| No trend tracking | Rolling revenue averages | Consistent visibility into momentum |

---

## ğŸ— **Architecture**

```
                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                          â”‚   Retail Sales Analytics Pipeline (ETL)  â”‚
                          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                           â”‚
                                           â–¼
                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                   â”‚   Synthetic Data Generator (Faker CSVs)      â”‚
                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                           â”‚
                                           â–¼
                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                   â”‚   Ingestion Layer (schema validation)        â”‚
                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                           â”‚
                                           â–¼
                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                   â”‚   Transformation Layer (KPIs, enrichment)    â”‚
                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                           â”‚
                                           â–¼
                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                   â”‚      SQLite Warehouse (indexed tables)       â”‚
                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚                           â”‚
                        â–¼                           â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚       FastAPI Service     â”‚   â”‚     Streamlit Dashboard      â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                   â”‚     APScheduler Automation (Daily ETL)       â”‚
                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                   â”‚ Monitoring, Drift Detection & Slack Alerts   â”‚
                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## **Project Structure**

```
retail_sales_pipeline/
â”‚   README.md
â”‚   requirements.txt
â”‚   schema_config.json
â”‚   run_all.py
â”‚
â”œâ”€â”€ dashboard/
â”‚       streamlit_app.py
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/
â”‚   â”œâ”€â”€ ingested/
â”‚   â”œâ”€â”€ processed/
â”‚   â””â”€â”€ product_catalog.csv
â”‚
â”œâ”€â”€ db/
â”‚       retail_sales.db
â”‚
â”œâ”€â”€ images/
â”‚       dashboard_kpis.png
â”‚       dashboard_region_chart.png
â”‚       dashboard_top_products.png
â”‚       dashboard_summary.png
â”‚       revenue_trends.png
â”‚       monitoring_log.png
â”‚       scheduler_run.png
â”‚
â”œâ”€â”€ logs/
â”‚       extract_sales.log
â”‚       generate_fake_sales.log
â”‚       load_to_db.log
â”‚       monitoring.log
â”‚       pipeline_health.json
â”‚       scheduler.log
â”‚       transform_sales.log
â”‚
â”œâ”€â”€ scripts/
â”‚       alerts.py
â”‚       api_server.py
â”‚       extract_sales.py
â”‚       generate_fake_sales.py
â”‚       transform_sales.py
â”‚       load_to_db.py
â”‚       monitoring.py
â”‚       scheduler.py
â”‚       __init__.py
â”‚
â””â”€â”€ tests/
```

---

## **Key Features**

### Automated ETL Pipeline  
Synth â†’ Ingest â†’ Transform â†’ Load â†’ Serve â†’ Visualize â†’ Monitor.

### Schema Validation  
Strict ingestion validation via `schema_config.json`.

### KPI Computation  
Revenue, cost, profit, margin%, grouped aggregations.

### SQLite Data Warehouse  
Indexed analytical tables for fast API and dashboard queries.

### FastAPI Service  
Exposes:
- `/health`
- `/kpi/revenue`
- `/kpi/top-products`

### Streamlit Dashboard  
Revenue trends, KPIs, regions, top products, rolling averages.

### Monitoring Module  
- Schema drift detection  
- Row count anomalies  
- Revenue deviation alerts  
- Runtime monitoring  
- Persistent monitoring table  

### lack Alerting  
Failure alerts + anomaly detection notifications.

---

## **Dashboard Samples**

### ğŸ”¹ KPI Overview
![](images/dashboard_kpis.png)

### ğŸ”¹ Revenue by Region
![](images/dashboard_region_chart.png)

### ğŸ”¹ Top Products
![](images/dashboard_top_products.png)

### ğŸ”¹ Rolling Revenue Trend
![](images/revenue_trends.png)

### ğŸ”¹ Summary Insights
![](images/dashboard_summary.png)

### ğŸ”¹ Monitoring Log
![](images/monitoring_log.png)

### ğŸ”¹ Scheduler Execution
![](images/scheduler_run.png)

---

## **Setup Guide**

### Install Dependencies
```bash
pip install -r requirements.txt
```

### Run ETL Manually
```bash
python scripts/generate_fake_sales.py
python scripts/extract_sales.py
python scripts/transform_sales.py
python scripts/load_to_db.py
```

### Launch FastAPI
```bash
uvicorn scripts.api_server:app --reload
```

### Start Dashboard
```bash
streamlit run dashboard/streamlit_app.py
```

### Start Automated Scheduler
```bash
python scripts/scheduler.py
```





**This project demonstrates a complete, production-style retail analytics data platform built entirely with local, lightweight technologies.**
